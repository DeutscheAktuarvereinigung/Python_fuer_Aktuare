{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/DeutscheAktuarvereinigung/Python_fuer_Aktuare/blob/main/examples/ClaimPrediction/insurance-claim-prediction-notebook.ipynb) [![Kaggle](https://kaggle.com/static/images/open-in-kaggle.svg)](https://kaggle.com/kernels/welcome?src=https://github.com/DeutscheAktuarvereinigung/Python_fuer_Aktuare/blob/main/examples/ClaimPrediction/insurance-claim-prediction-notebook.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the required libraries\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import warnings\n",
    "#warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the display to 100 rows and columns\n",
    "\n",
    "pd.set_option('display.max_rows',100)\n",
    "pd.set_option('display.max_columns',100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the Dataset \n",
    "\n",
    "#train_df = pd.read_csv('./train.csv')\n",
    "#test_df = pd.read_csv('./test.csv')\n",
    "train_df = pd.read_csv('https://raw.githubusercontent.com/DeutscheAktuarvereinigung/Python_fuer_Aktuare/refs/heads/main/examples/ClaimPrediction/train.csv')\n",
    "test_df = pd.read_csv('https://raw.githubusercontent.com/DeutscheAktuarvereinigung/Python_fuer_Aktuare/refs/heads/main/examples/ClaimPrediction/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Insights and EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing the shape of the train and test dataset\n",
    "\n",
    "train_df.shape, test_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the summary of the dataset\n",
    "\n",
    "train_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comments:\n",
    "\n",
    "- This info reflects that the dataset has no null values.\n",
    "- There are 28 Categorical features and 16 numerical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's check for missing values:\n",
    "\n",
    "train_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comments:\n",
    "- The dataset has no missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for duplicate values:\n",
    "\n",
    "train_df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comments:\n",
    "- We can see there is no duplicate data in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#View the statistical summary of numerical variables\n",
    "\n",
    "train_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#categorical features\n",
    "categorical = train_df.select_dtypes(include =[object])\n",
    "print(\"Categorical Features in DataSet:\",categorical.shape[1])\n",
    "print(categorical.columns)\n",
    "\n",
    "#numerical features\n",
    "numerical= train_df.select_dtypes(include =[np.float64,np.int64])\n",
    "print(\"Numerical Features in DataSet:\",numerical.shape[1])\n",
    "print(numerical.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.is_claim.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We have imbalanced data in Target column. We will handle that before feed to model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Univariate Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Numerical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = [i for i in numerical.columns]\n",
    "plt.figure(figsize=(10,15))\n",
    "for n,column in enumerate(target):\n",
    "    plot=plt.subplot(8,2,n+1)\n",
    "    sns.histplot(train_df[column],color='green')\n",
    "    plt.title(f'{column.title()}',weight='bold')\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Categorical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical=categorical.drop('policy_id',axis=1)\n",
    "\n",
    "target = [i for i in categorical.columns]\n",
    "plt.figure(figsize=(15,25))\n",
    "for n,column in enumerate(target):\n",
    "    plot=plt.subplot(14,2,n+1)\n",
    "    sns.countplot(train_df, x= column)\n",
    "    plt.title(f'{column.title()}',weight='bold')\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_cpy = train_df.copy()\n",
    "test_cpy = test_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_cpy.drop(['policy_id'], axis=1, inplace=True)\n",
    "test_cpy.drop(['policy_id'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ordinal_col = ['max_torque', 'max_power', 'transmission_type', 'steering_type']\n",
    "\n",
    "train_cpy['transmission_type'] = train_cpy['transmission_type'].replace({'Manual' : 1, 'Automatic' : 2})\n",
    "train_cpy['steering_type'] = train_cpy['steering_type'].replace({'Manual' : 1, 'Power' : 2, 'Electric': 3})\n",
    "\n",
    "train_cpy[['max_torque_Nm', 'max_torque_rpm']] = train_cpy[\"max_torque\"].apply(lambda x: pd.Series(str(x).split(\"@\")))\n",
    "train_cpy.drop([\"max_torque\"], axis=1, inplace= True)\n",
    "train_cpy['max_torque_Nm'] = train_cpy['max_torque_Nm'].str[:-2].astype(float)\n",
    "train_cpy['max_torque_rpm'] = train_cpy['max_torque_rpm'].str[:-3].astype(int)\n",
    "\n",
    "\n",
    "\n",
    "train_cpy[['max_power_bhp', 'max_power_rpm']] = train_cpy[\"max_power\"].apply(lambda x: pd.Series(str(x).split(\"@\")))\n",
    "train_cpy.drop([\"max_power\"], axis=1, inplace= True)\n",
    "train_cpy['max_power_rpm'] = train_cpy['max_power_rpm'].str[:-3].astype(int)\n",
    "train_cpy['max_power_bhp'] = train_cpy['max_power_bhp'].str[:-3].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_cpy['transmission_type'] = test_cpy['transmission_type'].replace({'Manual' : 1, 'Automatic' : 2})\n",
    "test_cpy['steering_type'] = test_cpy['steering_type'].replace({'Manual' : 1, 'Power' : 2, 'Electric': 3})\n",
    "\n",
    "test_cpy[['max_torque_Nm', 'max_torque_rpm']] = test_cpy[\"max_torque\"].apply(lambda x: pd.Series(str(x).split(\"@\")))\n",
    "test_cpy.drop([\"max_torque\"], axis=1, inplace= True)\n",
    "test_cpy['max_torque_Nm'] = test_cpy['max_torque_Nm'].str[:-2].astype(float)\n",
    "test_cpy['max_torque_rpm'] = test_cpy['max_torque_rpm'].str[:-3].astype(int)\n",
    "\n",
    "\n",
    "\n",
    "test_cpy[['max_power_bhp', 'max_power_rpm']] = test_cpy[\"max_power\"].apply(lambda x: pd.Series(str(x).split(\"@\")))\n",
    "test_cpy.drop([\"max_power\"], axis=1, inplace= True)\n",
    "test_cpy['max_power_rpm'] = test_cpy['max_power_rpm'].str[:-3].astype(int)\n",
    "test_cpy['max_power_bhp'] = test_cpy['max_power_bhp'].str[:-3].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_cpy = pd.get_dummies(train_cpy, drop_first=True)\n",
    "test_cpy = pd.get_dummies(test_cpy,drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train_cpy.drop(['is_claim'], axis=1)\n",
    "y = train_cpy['is_claim']\n",
    "X_test = test_cpy.copy()\n",
    "\n",
    "policy_id = test_df['policy_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## SMOTE to handle imbalanced data\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "sm =  SMOTE(random_state=12, sampling_strategy=0.8)\n",
    "\n",
    "X_sm, y_sm =  sm.fit_resample(X,y)\n",
    "\n",
    "X_sm.shape, y_sm.shape\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "print('Original dataset shape {}'.format(Counter(y)))\n",
    "print('Resampled dataset shape {}'.format(Counter(y_sm)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train,X_test,y_train,y_test=train_test_split(X_sm,y_sm,test_size = 0.2, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.metrics import accuracy_score,f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Building\n",
    "Takes some time, approx 1 - 2 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt=RandomForestClassifier(n_estimators=1000,\n",
    "                         criterion='gini',\n",
    "                         max_depth=12,\n",
    "                         max_features='log2',\n",
    "                         min_samples_leaf=1,\n",
    "                         min_samples_split=5,\n",
    "                         random_state=42)\n",
    "\n",
    "# Train Model\n",
    "dt.fit(X_train,y_train)\n",
    "\n",
    "# Training set performance\n",
    "train_accuracy= dt.score(X_train,y_train)\n",
    "\n",
    "#Testing set performance\n",
    "test_accuracy=dt.score(X_test,y_test)\n",
    "\n",
    "\n",
    "print('Accuracy for Training set is')\n",
    "print( 100*train_accuracy)\n",
    "print('----------------------------------')\n",
    "print('Accuracy for Testing set is')\n",
    "print( 100*test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = dt.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(f1_score(y_test,pred))\n",
    "print('\\n')\n",
    "print(classification_report(y_test,pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred=dt.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 2636290,
     "sourceId": 4511133,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30301,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
