{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python für Aktuare Teil 5\n",
    "\n",
    "## Agenda\n",
    "Innerhalb dieses Notebooks behandeln wir:\n",
    "- Einführung in maschinelles Lernen:\n",
    "    - Grundlegende Konzepte des maschinellen Lernens\n",
    "    - Supervised Learning: Regression und Klassifikation\n",
    "    - Unsupervised Learning: Clustering\n",
    "- Nutzung von Scikit-learn:\n",
    "    - Datenvorbereitung und Feature Engineering\n",
    "    - Training und Evaluierung von Modellen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Einführung in Maschinelles Lernen\n",
    "\n",
    "In diesem Abschnitt werden wir uns mit den grundlegenden Konzepten des **maschinellen Lernens** befassen. Maschinelles Lernen (ML) ist ein Teilgebiet der künstlichen Intelligenz, das es Computern ermöglicht, aus Daten zu lernen und Vorhersagen oder Entscheidungen zu treffen, ohne explizit programmiert zu sein.\n",
    "\n",
    "## 1. Grundlegende Konzepte des Maschinellen Lernens\n",
    "\n",
    "Maschinelles Lernen beruht auf dem Prinzip, dass Algorithmen aus **Daten** lernen, um Muster zu erkennen und auf Basis dieser Muster Vorhersagen oder Entscheidungen zu treffen. Die wichtigsten Begriffe und Konzepte dabei sind:\n",
    "\n",
    "- **Daten**: Die Grundlage für jedes maschinelle Lernmodell. Daten bestehen aus **Merkmalen** (Features), die für Vorhersagen oder Klassifizierungen relevant sind.\n",
    "- **Modelle**: Ein mathematisches Konstrukt, das aus den Daten lernt und Vorhersagen trifft.\n",
    "- **Training**: Der Prozess, in dem das Modell aus den vorhandenen Daten lernt.\n",
    "- **Überanpassung (Overfitting)**: Wenn ein Modell zu spezifisch für die Trainingsdaten ist und nicht gut auf neue, ungesehene Daten generalisiert.\n",
    "- **Evaluierung**: Der Prozess, das Modell zu testen, um zu sehen, wie gut es funktioniert.\n",
    "\n",
    "Es gibt verschiedene Arten von maschinellen Lernansätzen, die je nach Art der Daten und der Zielsetzung gewählt werden."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Supervised Learning: Regression und Klassifikation\n",
    "\n",
    "Beim **Supervised Learning** lernt das Modell von einem **beschrifteten Datensatz**, d.h. die Daten enthalten sowohl Eingaben als auch die zugehörigen Ausgaben. Das Ziel ist es, eine Funktion zu finden, die auf Grundlage der Eingaben die richtigen Ausgaben vorhersagt.\n",
    "\n",
    "### a. Regression\n",
    "\n",
    "Die **Regression** ist ein Verfahren im supervised learning, bei dem der Output eine kontinuierliche, numerische Größe ist. Das Modell versucht, eine Linie (oder Fläche) zu finden, die den Zusammenhang zwischen den Eingaben und der Ausgabe am besten beschreibt.\n",
    "\n",
    "Beispiele für Regression:\n",
    "- Vorhersage der Schadenshöhe bei Versicherungsfällen\n",
    "- Prognose von Aktienkursen\n",
    "\n",
    "Typische Algorithmen:\n",
    "- **Lineare Regression**\n",
    "- **Polynomiale Regression**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import der benötigten Bibliotheken\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Synthetische Daten erstellen\n",
    "np.random.seed(42)\n",
    "\n",
    "# Alter der Versicherungsnehmer (zufällig zwischen 18 und 70 Jahren)\n",
    "alter = np.random.randint(18, 70, 100)\n",
    "\n",
    "# Versicherungsprämie (angenommen, sie steigt linear mit dem Alter + etwas zufälliges Rauschen)\n",
    "prämie = 200 + (alter * 10) + np.random.normal(0, 100, 100)\n",
    "\n",
    "# Erstellen eines DataFrames\n",
    "daten = pd.DataFrame({'Alter': alter, 'Prämie': prämie})\n",
    "\n",
    "# Überblick über die Daten\n",
    "print(daten.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear Regression Modell\n",
    "model = LinearRegression()\n",
    "\n",
    "# Umformen der Daten für das Modell\n",
    "X = daten[['Alter']]  # Eingabedaten (Alter)\n",
    "y = daten['Prämie']   # Zielvariable (Prämie)\n",
    "\n",
    "# Modell trainieren\n",
    "model.fit(X, y)\n",
    "\n",
    "# Vorhersagen für die lineare Regression\n",
    "pred = model.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotten der Daten und der linearen Regression\n",
    "plt.scatter(alter, prämie, color='blue', label='Daten')\n",
    "plt.plot(alter, pred, color='red', label='Lineare Regression')\n",
    "\n",
    "# Beschriftungen und Titel\n",
    "plt.xlabel('Alter der Versicherungsnehmer')\n",
    "plt.ylabel('Versicherungsprämie')\n",
    "plt.title('Lineare Regression: Alter vs. Versicherungsprämie')\n",
    "plt.legend()\n",
    "\n",
    "# Plot anzeigen\n",
    "plt.show()\n",
    "\n",
    "# Ausgabe der Koeffizienten der linearen Regression\n",
    "print(f\"Koeffizient: {model.coef_[0]}\")\n",
    "print(f\"Intercept: {model.intercept_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. Klassifikation\n",
    "\n",
    "Die **Klassifikation** ist eine weitere Form des supervised learning, bei der der Output diskrete Klassen sind. Das Ziel ist es, die Daten in verschiedene Kategorien oder Gruppen einzuteilen.\n",
    "\n",
    "Beispiele für Klassifikation:\n",
    "- Erkennung von Betrugsfällen in Versicherungen\n",
    "- Vorhersage, ob ein Kunde ein Produkt kauft (Ja/Nein)\n",
    "\n",
    "Typische Algorithmen:\n",
    "- **Logistische Regression**\n",
    "- **Entscheidungsbäume**\n",
    "- **K-Nearest Neighbors (k-NN)**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import der benötigten Bibliotheken\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Synthetische Daten erstellen\n",
    "np.random.seed(42)\n",
    "\n",
    "# Alter der Versicherungsnehmer (zufällig zwischen 18 und 70 Jahren)\n",
    "alter = np.random.randint(18, 70, 100)\n",
    "\n",
    "# Versicherungsprämie (angenommen, sie steigt linear mit dem Alter + zufälliges Rauschen)\n",
    "prämie = 200 + (alter * 10) + np.random.normal(0, 100, 100)\n",
    "\n",
    "# Kundenklassen: Wir segmentieren die Kunden basierend auf einem Muster\n",
    "# (1 = Jung und geringe Prämie, 2 = Älter und mittlere Prämie, 3 = Älter und hohe Prämie)\n",
    "klassen = np.where((alter < 30) & (prämie < 500), 1,\n",
    "                   np.where((alter >= 30) & (alter < 50), 2, 3))\n",
    "\n",
    "# Erstellen eines DataFrames\n",
    "daten = pd.DataFrame({'Alter': alter, 'Prämie': prämie, 'Klasse': klassen})\n",
    "\n",
    "# Überblick über die Daten\n",
    "print(daten.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotten der Kunden und ihrer Klassen\n",
    "plt.scatter(daten['Alter'], daten['Prämie'], c=daten['Klasse'], cmap='viridis', label='Kundensegmente')\n",
    "plt.xlabel('Alter der Versicherungsnehmer')\n",
    "plt.ylabel('Versicherungsprämie')\n",
    "plt.title('K-Nearest Neighbors: Kundensegmentierung')\n",
    "plt.colorbar(label='Kundensegmente')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eingabe- und Zielvariablen\n",
    "X = daten[['Alter', 'Prämie']]\n",
    "y = daten['Klasse']\n",
    "\n",
    "# Daten in Trainings- und Testdaten aufteilen\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardisierung der Eingabedaten (wichtig für KNN)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# KNN-Klassifikator erstellen und trainieren\n",
    "knn = KNeighborsClassifier(n_neighbors=5)  # Wir wählen 5 Nachbarn\n",
    "knn.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Vorhersagen auf Testdaten\n",
    "y_pred = knn.predict(X_test_scaled)\n",
    "# Genauigkeit des Modells\n",
    "accuracy = knn.score(X_test_scaled, y_test)\n",
    "print(f\"Genauigkeit des Modells: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vergleich der Vorhersagen und tatsächlichen Klassen in zwei Subplots\n",
    "fig, ax = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Subplot 1: Tatsächliche Klassen\n",
    "scatter1 = ax[0].scatter(X_test['Alter'], X_test['Prämie'], c=y_test, marker='o', cmap='viridis')\n",
    "ax[0].set_title('Tatsächliche Klassen')\n",
    "ax[0].set_xlabel('Alter der Versicherungsnehmer')\n",
    "ax[0].set_ylabel('Versicherungsprämie')\n",
    "fig.colorbar(scatter1, ax=ax[0])\n",
    "\n",
    "# Subplot 2: Vorhergesagte Klassen\n",
    "scatter2 = ax[1].scatter(X_test['Alter'], X_test['Prämie'], c=y_pred, marker='x', cmap='viridis')\n",
    "ax[1].set_title('Vorhergesagte Klassen')\n",
    "ax[1].set_xlabel('Alter der Versicherungsnehmer')\n",
    "ax[1].set_ylabel('Versicherungsprämie')\n",
    "fig.colorbar(scatter2, ax=ax[1])\n",
    "\n",
    "# Rote Kreise um fehlerhafte Vorhersagen\n",
    "ax[1].scatter(X_test['Alter'][y_test != y_pred], X_test['Prämie'][y_test != y_pred], \n",
    "                      facecolors='none', edgecolors='r', s=150, linewidths=1, label='Fehler')\n",
    "\n",
    "\n",
    "# Layout-Anpassung\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Unsupervised Learning: Clustering\n",
    "\n",
    "Im Gegensatz zum supervised learning arbeitet **unsupervised learning** mit unbeschrifteten Daten. Das bedeutet, dass das Modell keine vordefinierten Ausgaben hat, sondern eigenständig Muster und Strukturen in den Daten findet.\n",
    "\n",
    "### Clustering\n",
    "\n",
    "Beim **Clustering** werden Datenpunkte in Gruppen unterteilt, die sich durch Ähnlichkeiten oder bestimmte gemeinsame Merkmale auszeichnen. Ziel ist es, Daten so zu gruppieren, dass Punkte innerhalb einer Gruppe möglichst ähnlich und Punkte aus verschiedenen Gruppen möglichst unterschiedlich sind.\n",
    "\n",
    "Beispiele für Clustering:\n",
    "- Segmentierung von Kunden in verschiedene Gruppen basierend auf ihrem Verhalten\n",
    "- Erkennung von Schadensmustern in Versicherungsdaten\n",
    "\n",
    "Typische Algorithmen:\n",
    "- **K-Means Clustering**\n",
    "- **Hierarchisches Clustering**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Auswahl der Merkmale für das Clustering (z.B. Alter und Prämie)\n",
    "X_clustering = X_test[['Alter', 'Prämie']]\n",
    "\n",
    "# K-Means Modell mit 3 Clustern\n",
    "kmeans = KMeans(n_clusters=3, random_state=42, n_init= 10)\n",
    "kmeans.fit(X_clustering)\n",
    "\n",
    "# Cluster-Zuordnungen vorhersagen\n",
    "cluster_labels = kmeans.predict(X_clustering)\n",
    "\n",
    "# Streudiagramm für das Clustering-Ergebnis\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(X_clustering['Alter'], X_clustering['Prämie'], c=cluster_labels, cmap='viridis', marker='o', s=100)\n",
    "plt.title('Kunden-Segmentierung mit K-Means Clustering')\n",
    "plt.xlabel('Alter der Versicherungsnehmer')\n",
    "plt.ylabel('Versicherungsprämie')\n",
    "\n",
    "# Cluster-Zentren plotten\n",
    "centroids = kmeans.cluster_centers_\n",
    "plt.scatter(centroids[:, 0], centroids[:, 1], c='red', marker='x', s=200, label='Cluster-Zentren')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zusammenfassung\n",
    "\n",
    "- Beim **supervised learning** lernt ein Modell von beschrifteten Daten, um Vorhersagen zu treffen.\n",
    "- Bei der **Regression** handelt es sich um die Vorhersage von kontinuierlichen Werten, bei der **Klassifikation** um die Zuordnung von Daten zu diskreten Klassen.\n",
    "- Beim **unsupervised learning** lernt das Modell ohne vorher definierte Ausgaben, und das **Clustering** ist eine Möglichkeit, ähnliche Datenpunkte zu gruppieren.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Datengetriebene Entscheidungen durch MachineLearning\n",
    "\n",
    "## Erstes Beispiel für datengetriebene Entscheidungen\n",
    "Wir befinden uns in der Geschäftseinheit für Naturgefahrenversicherungen.\n",
    "\n",
    "**Ziel**: Müssen wir unsere Prämien in den nächsten drei Jahren erhöhen?\n",
    "\n",
    "-> Können wir ein Modell erstellen, das unsere Schadensmeldungen abbildet?\n",
    "\n",
    "**Daten**:\n",
    "- Wetterdaten\n",
    "- Öffentlich verfügbare Finanzdaten\n",
    "- Schadensdaten der letzten fünf Jahre\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Daten laden und erster Blick auf die Daten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "weather_data = pd.read_csv('./data/weather_data.csv',parse_dates=['Date'])\n",
    "finance_data = pd.read_csv('./data/financial_data.csv',parse_dates=['Date'])\n",
    "claims_data = pd.read_csv('./data/claims_data.csv',parse_dates=['Date'])\n",
    "# from remote server\n",
    "#weather_data = pd.read_csv('https://raw.githubusercontent.com/DeutscheAktuarvereinigung/Python_fuer_Aktuare/refs/heads/main/data/weather_data.csv',parse_dates=['Date'])\n",
    "#finance_data = pd.read_csv('https://raw.githubusercontent.com/DeutscheAktuarvereinigung/Python_fuer_Aktuare/refs/heads/main/data/financial_data.csv',parse_dates=['Date'])\n",
    "#claims_data  = pd.read_csv('https://raw.githubusercontent.com/DeutscheAktuarvereinigung/Python_fuer_Aktuare/refs/heads/main/data/claims_data.csv',parse_dates=['Date'])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wetter Daten\n",
    "Schauen wir uns einmal die Wetterdaten an."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the data\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Plot temperature\n",
    "plt.subplot(3, 1, 1)\n",
    "plt.plot(weather_data['Date'], weather_data['Temperature (Celsius)'], label='Temperature (Celsius)')\n",
    "plt.title('Weather Data Visualization')\n",
    "plt.ylabel('Temperature (Celsius)')\n",
    "plt.legend()\n",
    "\n",
    "# Plot humidity\n",
    "plt.subplot(3, 1, 2)\n",
    "plt.bar(weather_data['Date'], weather_data['Humidity (%)'], label='Humidity (%)', color='orange')\n",
    "plt.ylabel('Humidity (%)')\n",
    "plt.legend()\n",
    "\n",
    "# Plot precipitation\n",
    "plt.subplot(3, 1, 3)\n",
    "plt.bar(weather_data['Date'], weather_data['Precipitation (mm)'], label='Precipitation (mm)', color='green')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Precipitation (mm)')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finanzdaten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting MSCI World Index, Inflation Rate, and GDP Growth\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# MSCI World Index\n",
    "plt.subplot(3, 1, 1)\n",
    "plt.plot(finance_data['Date'], finance_data['Stock Index'], label='Stock Index', color='blue')\n",
    "plt.title('Stock Index Over Time')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Index Value')\n",
    "plt.legend()\n",
    "\n",
    "# Inflation Rate\n",
    "plt.subplot(3, 1, 2)\n",
    "plt.plot(finance_data['Date'], finance_data['Inflation Rate'], label='Inflation Rate', color='orange')\n",
    "plt.title('Inflation Rate Over Time')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Inflation Rate')\n",
    "plt.legend()\n",
    "\n",
    "# GDP Growth\n",
    "plt.subplot(3, 1, 3)\n",
    "plt.plot(finance_data['Date'], finance_data['GDP Growth'], label='GDP Growth', color='green')\n",
    "plt.title('GDP Growth Over Time')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('GDP Growth Rate')\n",
    "plt.legend()\n",
    "\n",
    "# Adjust layout for better spacing\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Schadendaten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the distribution of Claim Amounts\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.hist(claims_data['ClaimAmount'], bins=50, color='skyblue', edgecolor='black')\n",
    "plt.title('Distribution of Claim Amounts')\n",
    "plt.xlabel('Claim Amount')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by Location and ClaimType and count the number of claims\n",
    "claims_grouped = claims_data.groupby(['Location', 'ClaimType']).size().unstack(fill_value=0)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(12, 8))\n",
    "claims_grouped.plot(kind='bar', stacked=True)\n",
    "plt.title('Distribution of Claims by Location and Claim Type')\n",
    "plt.xlabel('Location')\n",
    "plt.ylabel('Count of Claims')\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend(title='Claim Type')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show histplot for every ClaimType and Location\n",
    "locations = ['City', 'Rural', 'Suburb']\n",
    "claim_types = [\"Flood\", \"Earthquake\", \"Storm\"]\n",
    "fig, axes = plt.subplots(len(locations), len(claim_types), figsize=(15, 15), sharex=True, sharey=True)\n",
    "\n",
    "# iterate over every location and type\n",
    "for i, location in enumerate(locations):\n",
    "    for j, claim_type in enumerate(claim_types):\n",
    "        # Filtern Sie die Daten entsprechend der aktuellen Kombination von Standort und Schadentyp\n",
    "        subset = claims_data[(claims_data['Location'] == location) & (claims_data['ClaimType'] == claim_type)]\n",
    "        # Erstellen Sie ein Histogramm für die Schadensbeträge in diesem Subset\n",
    "        axes[i, j].hist(subset['ClaimAmount'], bins=20, color='skyblue', edgecolor='black')\n",
    "        # Einstellungen für die Beschriftung und den Titel\n",
    "        axes[i, j].set_title(f'Location: {location}, Claim Type: {claim_type}')\n",
    "        axes[i, j].set_xlabel('Claim Amount')\n",
    "        axes[i, j].set_ylabel('Frequency')\n",
    "\n",
    "# Layout\n",
    "for ax in axes.flat:\n",
    "    ax.set_xlabel('Claim Amount')\n",
    "    ax.set_xticks(np.arange(10000, claims_data['ClaimAmount'].max(), 5000))\n",
    "    ax.set_xticklabels(np.arange(10000, claims_data['ClaimAmount'].max(), 5000), rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Daten vorbereiten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# erstmal alles in ein Datenset packen\n",
    "merged_data = pd.merge(weather_data, finance_data)\n",
    "merged_data = pd.merge(merged_data,claims_data)\n",
    "merged_data['DayOfYear'] = merged_data['Date'].dt.day_of_year\n",
    "merged_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wir wollen die Schadenhöhe aus den anderen Daten vorhersagen\n",
    "X = merged_data[['Temperature (Celsius)', 'Humidity (%)', 'Precipitation (mm)','Stock Index', 'Inflation Rate', 'GDP Growth']]\n",
    "y = merged_data['ClaimAmount']\n",
    "\n",
    "# Dateien in zwei Datensets aufteilen\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wir fitten ein einfaches Model\n",
    "from sklearn.metrics import mean_squared_error\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Vorhersagen auf dem Testset\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "\n",
    "# Modell evaluieren\n",
    "mse = mean_squared_error(y_test, predictions)\n",
    "print(f'Mean Squared Error: {mse}')\n",
    "\n",
    "r2_score = model.score(X_test, y_test)\n",
    "print(f'R-squared Score: {r2_score}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the slope (m) and intercept (b) of the regression line\n",
    "slope, intercept = np.polyfit(y_test, predictions, 1)\n",
    "\n",
    "# Create a figure with two subplots\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Plot the scatter plot with regression line in the first subplot\n",
    "ax1.scatter(y_test, predictions, label='Actual vs Predicted')\n",
    "ax1.plot(y_test, slope*y_test + intercept, color='red', label='Regression Line')\n",
    "ax1.plot(y_test, y_test, color='green', linestyle='--', label='Diagonal Line')\n",
    "# Add labels and title\n",
    "ax1.set_xlabel('Actual Values')\n",
    "ax1.set_ylabel('Predicted Values')\n",
    "ax1.set_title('Actual vs Predicted Values with Regression Line')\n",
    "ax1.legend()\n",
    "\n",
    "# Plot the residual plot in the second subplot\n",
    "residuals = y_test - predictions\n",
    "ax2.scatter(predictions, residuals)\n",
    "ax2.set_xlabel('Predicted Values')\n",
    "ax2.set_ylabel('Residuals')\n",
    "ax2.set_title('Residual Plot')\n",
    "ax2.axhline(y=0, color='r', linestyle='-')\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show plots\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "„Hmm, es sieht so aus, als wäre etwas schiefgelaufen. Ein gutes Modell wäre, wenn die rote Linie und die gepunktete grüne Linie besser übereinstimmen und die Residuen nahe Null liegen. Warten Sie mal.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Was ist mit Schadensart und Schadensort?\n",
    "Nun, das sind kategoriale Werte („Erdbeben“, „Überschwemmung“, ...). Aber die meisten maschinellen Lernalgorithmen erwarten nur numerische Werte. Bei unserem ersten Versuch haben wir sie ignoriert. Aber wie können wir sie verwenden? Zum Beispiel, wie bringen wir sie in numerische Werte?\n",
    "\n",
    "Eine Möglichkeit ist *One-Hot-Encoding*:\n",
    "\n",
    "Wir erstellen eine neue Spalte für jeden Wert in der kategorialen Spalte und markieren sie mit 1 / Wahr, wenn der Wert den Merkmalen entspricht, oder 0 / Falsch, wenn nicht. Zum Beispiel:\n",
    "\n",
    "| ClaimType | ClaimType_Earthquake | ClaimType_Flood | ClaimType_Storm |\n",
    "|-----------|----------------------|-----------------|-----------------|\n",
    "| Earthquake | 1                    | 0               | 0               |\n",
    "| Flood      | 0                    | 1               | 0               |\n",
    "| Storm      | 0                    | 0               | 1               |\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data_encoded = pd.get_dummies(merged_data, columns=['ClaimType', 'Location'])\n",
    "\n",
    "merged_data_encoded.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also auf ein neues, modellieren und evaluieren:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = merged_data_encoded.drop(['ClaimAmount', 'Date'], axis=1)\n",
    "y = merged_data['ClaimAmount']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "model_adv = LinearRegression()\n",
    "model_adv.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "predictions = model_adv.predict(X_test)\n",
    "\n",
    "\n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(y_test, predictions)\n",
    "print(f'Mean Squared Error: {mse}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the slope (m) and intercept (b) of the regression line\n",
    "slope, intercept = np.polyfit(y_test, predictions, 1)\n",
    "\n",
    "# Create a figure with two subplots\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Plot the scatter plot with regression line in the first subplot\n",
    "ax1.scatter(y_test, predictions, label='Actual vs Predicted')\n",
    "ax1.plot(y_test, slope*y_test + intercept, color='red', label='Regression Line')\n",
    "ax1.plot(y_test, y_test, color='green', linestyle='--', label='Diagonal Line')\n",
    "# Add labels and title\n",
    "ax1.set_xlabel('Actual Values')\n",
    "ax1.set_ylabel('Predicted Values')\n",
    "ax1.set_title('Actual vs Predicted Values with Regression Line')\n",
    "ax1.legend()\n",
    "\n",
    "# Plot the residual plot in the second subplot\n",
    "residuals = y_test - predictions\n",
    "ax2.scatter(predictions, residuals)\n",
    "ax2.set_xlabel('Predicted Values')\n",
    "ax2.set_ylabel('Residuals')\n",
    "ax2.set_title('Residual Plot')\n",
    "ax2.axhline(y=0, color='r', linestyle='-')\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show plots\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Viel besser! Eigentlich... ein bisschen zu gut. Es scheint fast wie eine perfekte Anpassung. Nun ... Da dies künstliche Daten sind, habe ich ein wenig geschummelt und die Schadensbeträge mit einer linearen Regressionsfunktion und etwas stochastischem Rauschen generiert, sodass das Modell perfekt funktioniert."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aufgabe\n",
    "\n",
    "Jetzt sind wieder Sie dran. Im Ordner `examples` finden Sie drei Machine-Learning-Notebooks übernommen aus kaggle. Sie sollten nun in der Lage sein, sich diese anzusehen und *durchzuklicken*. Eventuell schaffen Sie es ja, diese an der ein oder anderen Stelle zu erweitern."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abschluss des Seminars\n",
    "\n",
    "Herzlichen Dank für Ihre Teilnahme am Seminar! Wir haben gemeinsam einen umfassenden Überblick über die verschiedenen Aspekte der Datenanalyse und Visualisierung gewonnen. \n",
    "\n",
    "Wir begannen mit den **Python-Grundlagen**, die Ihnen die notwendigen Werkzeuge an die Hand gegeben haben, um Daten zu manipulieren und zu analysieren. Anschließend haben wir uns intensiv mit **Pandas** beschäftigt, um die Grundlagen der Datenverarbeitung zu erlernen, einschließlich des Umgangs mit fehlenden Werten und der Erstellung von Pivot-Tabellen.\n",
    "\n",
    "In der nächsten Phase haben wir verschiedene **Visualisierungstechniken** behandelt. Wir haben gelernt, wie man aussagekräftige Diagramme mit **Matplotlib** und **Seaborn** erstellt, um Erkenntnisse aus Daten zu gewinnen. \n",
    "\n",
    "Darüber hinaus haben wir uns mit den Grundlagen des **maschinellen Lernens** beschäftigt. Wir haben Konzepte wie **Supervised Learning** und **Unsupervised Learning** erkundet und einfache Modelle wie **lineare Regression** und **k-nearest neighbors** implementiert.\n",
    "\n",
    "Abschließend hoffe ich, dass Sie wertvolle Fähigkeiten und Kenntnisse erworben haben, die Ihnen helfen werden, datenbasierte Entscheidungen in Ihrem Arbeitsumfeld zu treffen. Denken Sie daran, dass die Welt der Datenanalyse und des maschinellen Lernens ständig wächst und sich weiterentwickelt. Bleiben Sie neugierig und motiviert Ihre Kenntnisse weiter auszubauen!\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
